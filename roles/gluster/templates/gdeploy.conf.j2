#gdeploy configuration generated by cockpit-gluster plugin
[hosts]
{% for rhhi_host in rhhi_host_list %}
{{ rhhi_host['glu_ip'] }}
{% endfor %}

[script1]
action=execute
ignore_script_errors=no
file=/usr/share/ansible/gdeploy/scripts/grafton-sanity-check.sh -d {{ unique_brick_device_names | join(',') }} -h {{ rhhi_host_list | join(',', attribute='glu_ip') }}

[disktype]
{{ gluster_raid_type }}

[diskcount]
{{ gluster_raid_data_disk_count }}

[stripesize]
{{ gluster_raid_stripe_size }}

[service1]
action=enable
service=chronyd

[service2]
action=restart
service=chronyd

[shell2]
action=execute
command=vdsm-tool configure --force

[script3]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-multipath.sh
ignore_script_errors=no

{% for device_name in unique_brick_device_names %}
[pv{{ loop.index }}]
action=create
devices={{ device_name }}
ignore_pv_errors=no

{% endfor %}
{% for device_name in unique_brick_device_names %}
[vg{{ loop.index }}]
action=create
vgname=gluster_vg_{{ device_name }}
pvname={{ device_name }}
ignore_vg_errors=no

{% endfor %}

[lv1]
action=create
poolname=gluster_thinpool_sdb
ignore_lv_errors=no
vgname=gluster_vg_sdb
lvtype=thinpool
size=21GB
poolmetadatasize=1GB

[lv2]
action=create
lvname=gluster_lv_engine
ignore_lv_errors=no
vgname=gluster_vg_sdb
mount=/gluster_bricks/engine
size=100GB
lvtype=thick

[lv3]
action=create
lvname=gluster_lv_data
ignore_lv_errors=no
vgname=gluster_vg_sdb
mount=/gluster_bricks/data
lvtype=thinlv
poolname=gluster_thinpool_sdb
virtualsize=10GB

[lv4]
action=create
lvname=gluster_lv_vmstore
ignore_lv_errors=no
vgname=gluster_vg_sdb
mount=/gluster_bricks/vmstore
lvtype=thinlv
poolname=gluster_thinpool_sdb
virtualsize=10GB

[selinux]
yes

[service3]
action=restart
service=glusterd
slice_setup=yes

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp,54322/tcp
services=glusterfs

[script2]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-gluster-hooks.sh

[shell3]
action=execute
command=usermod -a -G gluster qemu

[volume1]
action=create
volname=engine
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
brick_dirs={{ rhhi_host_list | map(attribute='glu_ip') | map('regex_replace', '^(.*)$', '\\1:/gluster_bricks/engine/engine' ) | list | join(',') }}
ignore_volume_errors=no

[volume2]
action=create
volname=data
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
brick_dirs={{ rhhi_host_list | map(attribute='glu_ip') | map('regex_replace', '^(.*)$', '\\1:/gluster_bricks/data/data' ) | list | join(',') }}
ignore_volume_errors=no

[volume3]
action=create
volname=vmstore
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
brick_dirs={{ rhhi_host_list | map(attribute='glu_ip') | map('regex_replace', '^(.*)$', '\\1:/gluster_bricks/vmstore/vmstore' ) | list | join(',') }}
ignore_volume_errors=no
